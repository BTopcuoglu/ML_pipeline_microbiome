---
title: "Hyperparameter tuning"
author: "Begüm D. Topçuoğlu"
output: rmarkdown::html_vignette
bibliography: paper.bib
vignette: >
  %\VignetteIndexEntry{Hyperparameter tuning}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

One particularly important aspect of ML is hyperparameter tuning. 
We must explore a range of hyperparameter possibilities to pick the ideal value for the model and dataset. In this package, we do this during the cross-validation step.

Let's start with an example ML run. 
The input data to `run_ml()` is a dataframe where each row is a sample or observation.
One column (assumed to be the first) is the outcome of interest,
and all of the other columns are the features.
We package `otu_mini_bin` as a small example dataset with `mikropml`.

```{r}
#install.packages("devtools")
#devtools::install_github("SchlossLab/mikropml")
library(mikropml)
head(otu_mini_bin)
```

Before we run ML, we can preprocess the data. 
You can learn more about this in the preprocessing vignette: `vignette(preprocess)`.

```{r}
preproc <- preprocess_data(dataset = otu_mini_bin,
                           outcome_colname = 'dx')
dat <- preproc$dat_transformed
```

We'll use `dat` for the following examples.

# The simplest way to `run_ml()`

As mentioned above, the minimal input is your dataset (`dataset`) and the machine learning model you want to use (`method`).

When we `run_ml()`, by default we do a 100 times repeated, 5-fold cross-validation, where we evaluate the hyperparameters in these 500 total iterations.

Say we want to run L2 regularized logistic regression. 

We run ML with:

```{r}
results <- run_ml(dat,
                  'glmnet',
                  outcome_colname = 'dx',
                  cv_times = 100, 
                  seed = 2019)
```

There was a warning. It says that where is something weird going on with the cross-validation step. Apparently some of the hyperparameters aren't a good fit. We should fix that. How do we do this?

By default, we provide the `run_ML` function with pre-determined parameters and hyperparameters. As you can see the `alpha` is set to 0. This is to make sure we are running L2 regularization. `glmnet` gives us the option to run both L1 and L2 regularization. If we change `alpha` to 1, we would have been running L1-regularized logistic regression. The hyperparameter `lambda` which adjusts the L2 regularization penalty is a range of values between 10^4 to 10. 

```{r}
default_glmnet <- set_hparams_glmnet()
default_glmnet

```

```{r}
results$trained_model
```

When we look at the 100 repeated cross-validation performance metrics such as `AUC`, `Accuracy`, `prAUC` for each tested `lambda` value, we see that some are not appropriate for this dataset and some do better than others. 

We want to change the `lambda` values to provide a better range to test in cross-validation step. We don't want to use the defaults but provide our own dataframe with new set of values. 

This dataframe hyperparameters should have columns `param`, `value`, and `method`.

For example:
```{r}
hparams_df <- dplyr::tibble(
  param = c("alpha", "lambda", "lambda", "lambda",  "lambda",  "lambda", "lambda", "lambda", "lambda"),
  value = c(1, 10^-2, 5*10^-2,10^-3, 10^-4, 10^-5, 10^-6, 10^-7, 10^-8),
  method = rep("glmnet", 9)
)

hparams_df

new_hp <- get_hyperparams_from_df(hparams_df, "glmnet")
new_hp


results <- run_ml(dat,
                  'glmnet',
                  outcome_colname = 'dx',
                  cv_times = 100,
                  hyperparameters = new_hp,
                  seed = 2019
                  )
```


# References
