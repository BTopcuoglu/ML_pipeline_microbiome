---
title: "Hyperparameter tuning"
author: "Begüm D. Topçuoğlu"
output: rmarkdown::html_vignette
bibliography: paper.bib
vignette: >
  %\VignetteIndexEntry{Hyperparameter tuning}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

One particularly important aspect of ML is hyperparameter tuning. 
We must explore a range of hyperparameter possibilities to pick the ideal value for the model and dataset. In this package, we do this during the cross-validation step.

Let's start with an example ML run. 
The input data to `run_ml()` is a dataframe where each row is a sample or observation.
One column (assumed to be the first) is the outcome of interest,
and all of the other columns are the features.
We package `otu_mini_bin` as a small example dataset with `mikropml`.

```{r}
#install.packages("devtools")
#devtools::install_github("SchlossLab/mikropml")
library(mikropml)
head(otu_mini_bin)
```

Before we run ML, we can preprocess the data. 
You can learn more about this in the preprocessing vignette: `vignette(preprocess)`.

```{r}
preproc <- preprocess_data(dataset = otu_mini_bin,
                           outcome_colname = 'dx')
dat <- preproc$dat_transformed
```

We'll use `dat` for the following examples.

# The simplest way to `run_ml()`

As mentioned above, the minimal input is your dataset (`dataset`) and the machine learning model you want to use (`method`).

When we `run_ml()`, by default we do a 100 times repeated, 5-fold cross-validation, where we evaluate the hyperparameters in these 500 total iterations.

Say we want to run L2 regularized logistic regression. 

We run ML with:

```{r}
results <- run_ml(dat,
                  'glmnet',
                  outcome_colname = 'dx',
                  cv_times = 100, 
                  seed = 2019)
```

By default, we provide the `run_ML` function with pre-determined parameters and hyperparameters.
As you can see the `alpha` is set to 0. This is to make sure we are running L2 regularization. `glmnet` gives us the option to run both L1 and L2 regularization. 
If we change `alpha` to 1, we would have been running L1-regularized logistic regression. The default hyperparameter `lambda` which adjusts the L2 regularization penalty is a range of values between 10^4 to 10. 

```{r}
default_glmnet <- set_hparams_glmnet()
default_glmnet
```


When we look at the 100 repeated cross-validation performance metrics such as `AUC`, `Accuracy`, `prAUC` for each tested `lambda` value, we see that some are not appropriate for this dataset and some do better than others. 

```{r}
results$trained_model
```

We want to change the `lambda` values to provide a better range to test in cross-validation step. We don't want to use the defaults but provide our own dataframe with new set of values. 

This dataframe hyperparameters should have columns `param`, `value`, and `method`.

For example:
```{r}
hparams_df <- dplyr::tibble(
  param = c("alpha", "lambda", "lambda", "lambda",  "lambda",  "lambda", "lambda", "lambda", "lambda"),
  value = c(1, 10^-2, 0.015, 0.02, 0.025, 0.03, 0.04, 0.05, 0.06),
  method = rep("glmnet", 9)
)
hparams_df

new_hp <- get_hyperparams_from_df(hparams_df, "glmnet")
new_hp
```

Let's run L2 logistic regression with the new `lamda` settings:

```{r}
results <- run_ml(dat,
                  'glmnet',
                  outcome_colname = 'dx',
                  cv_times = 100,
                  hyperparameters = new_hp,
                  seed = 2019
                  )
results$trained_model
```

This time, we cover a different range of `lambda` settings in cross-validation. 

How do we know which `lambda` value is the best one? 
To answer that, we need to run the ML pipeline on multiple datasplits and look at the mean cross-validation performance of each `lambda` across those modeling experiments. 
We describe how to run ML with multiple datasplits here: http://www.schlosslab.org/mikropml/articles/parallel.html

Here we run ML on 3 dataplits with the new `lambda` range we defined above. 
We can then use `plot_hp_performance` to see which `lambda` gives us the largest mean AUC value across modeling experiments. 

```{r}
results <- lapply(seq(100, 102), function(seed) {
   run_ml(dat, "glmnet", seed = seed, hyperparameters = new_hp)
 })
models <- lapply(results, function(x) x$trained_model)
hp_metrics <- combine_hp_performance(models)
plot_hp_performance(hp_metrics$dat, lambda, AUC)
```

As you can see, we get a mean maxima at 0.3 which is the best `lambda` value for this dataset when we run 3 datasplits. This value is used for building the final model. For a better understanding of the global maxima, it would be better to run more datasplits. 

# Other methods

Here are the hyperparameters that are tuned for each of the modeling methods.
The output for all of them is very similar, so we won't go into those details.

## Random forest

When we run the `rf` method, we are tuning the `mtry` hyperparameter. This is the number of features that are randomly collected to be sampled at each tree node. It needs to be an appropriate number in relation to the number of features in the dataset.

By default, we take the square root of number of features in the dataset and we provide a range that is
`[sqrt_features / 2, sqrt_features, sqrt_features * 2]`. 

For example if the number of features is 1000:

```{r}
set_hparams_rf(1000)
```

Similar to `glmnet` method, we can provide our own `mtry` range.

## Decision tree

When we run the `rpart2` method, we are tuning the `maxdepth` hyperparameter. This is the maximum depth of any node of the final tree.

By default, we provide a range that is less than the number of features in the dataset. 

For example if we have 1000 features:

```{r}
set_hparams_rpart2(1000)
```
For example if we have 100 features:
```{r}
set_hparams_rpart2(50)
```
For example if we have 10 features:
```{r}
set_hparams_rpart2(10)
```


## SVM

When we run the `svmRadial` method, we are tuning the `C` and `sigma` hyperparameters. 
`sigma` defines how far the influence of a single training example reaches and `C` behaves as a regularization parameter.

By default, we provide 2 separate range of values for the two hyperparameters. 


```{r}
set_hparams_svmRadial()
```

## XGBoost

When we run the `xgbTree` method, we are tuning the `nrounds`, `gamma`, `eta` `max_depth`, `colsample_bytree`, `min_child_weight` and `subsample` hyperparameters. Like other tree-based methods some of these hyperparameters need to be appropriate in relation to the number of features in the dataset.

By default, we set the `nrounds`, `gamma`, `colsample_bytree` and `min_child_weight` to fixed values and we provide a range of values for `eta`, `max_depth` and `subsample`. All of these can be changed and optimized by the user. 

```{r}
set_hparams_xgbTree(1000)
```

# References
