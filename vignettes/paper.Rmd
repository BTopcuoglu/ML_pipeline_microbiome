---
title: "mikropml: A User-Friendly Machine Learning R Package for Binary Classification Problems"
output: 
  rmarkdown::html_vignette:
    keep_md: true
tags:
  - R
  - machine learning
  - logistic regression
  - decision tree
  - random forest
  - xgBoost
  - microbiology
authors:
  - name: Begüm D. Topçuoğlu
    orcid: 0000-0003-3140-537X
    affiliation: 1;4
  - name: Zena Lapp
    orcid: 0000-0003-4674-2176
    affiliation: 1
  - name: Kelly L. Sovacool
    orcid: 0000-0003-3283-829X
    affiliation: 1
  - name: Evan Snitkin
    orcid: 0000-0001-8409-278X
    affiliation: 3;5
  - name: Jenna Wiens
    orcid: 0000-0002-1057-7722
    affiliation: 2
  - name: Patrick D. Schloss
    orcid: 0000-0002-6935-4275
    affiliation: 3
affiliations:
  - name: Department of Computational Medicine & Bioinformatics, University of Michigan
    index: 1
  - name: Department of Electrical Engineering & Computer Science, University of Michigan
    index: 2
  - name: Department of Microbiology & Immunology, University of Michigan
    index: 3
  - name: Exploratory Science Center, Merck & Co., Inc., Cambridge, Massachusetts, USA.
    index: 4
  - name: Department of Internal Medicine/Division of Infectious Diseases, University of Michigan
    index: 5
date: 2020
bibliography: paper.bib
vignette: >
  %\VignetteIndexEntry{mikropml paper}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
options(rmarkdown.html_vignette.check_title = FALSE)
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```
```{r render, eval = FALSE, echo = FALSE}
rmarkdown::render(here::here('vignettes','paper.Rmd'))
```

# Summary

Machine learning (ML) for classification of data into groups based on a set of features is used to make decisions in healthcare, economics, criminal justice and more. 
However, implementing a robust ML classification pipeline can be time-consuming, confusing, and difficult. 
Here, we present `mikropml` (prononced "meek-ROPE em el"), an easy-to-use R package that implements robust ML pipelines for binary classification problems using logistic regression, support vector machines, decision trees, random forest, or gradient boosted trees.
<!-- We need to have a discussion about whether we're only doing binary classification problems, or if we're expanding it to others as well -->
It is available on [GitHub](https://github.com/SchlossLab/mikropml/) and CRAN [**link to CRAN**]. 

# Statement of need

A robust machine learning (ML) pipeline requires data pre-processing, cross-validation, testing, model evaluation, and often interpretation of why the model makes particular predictions. 
Performing these steps using the correct methodology is extremely important, as failure to implement them can result in incorrect and misleading results [@teschendorff_avoiding_2019; @wiens_no_2019]. 

Supervised ML is widely used to recognize patterns in large datasets and to make predictions to categorize data. 
Several packages including `caret` [@kuhn_building_2008] and `tidymodels` [@kuhn_tidymodels_2020] in R and `scikitlearn` [@pedregosa_scikit-learn_2011] in Python allow scientists to train ML models with a variety of algorithms. 
While these packages provide all of the tools necessary for each ML step, they do not implement a complete robust ML pipeline according to best practices in the literature. 
This, paired with the vast number of options available, make it difficult for non-experts to easily perform robust ML analyses using these packages. 
Furthermore, these packages do not offer a unified way to identify features that contribute to improved model performance.

To enable a broader range of researchers to perform robust ML analyses, we created [`mikropml`](https://github.com/SchlossLab/mikropml/), an easy-to-use package in R [@r_core_team_r_2020] that implements the ML framework for binary classification problems created by Topçuoğlu _et al._ [@topcuoglu_framework_2020]. 
`mikropml` leverages the R `caret` package to support five different ML algorithms: logistic regression, support vector machine with a radial basis kernel, decision tree , random forest, and gradient boosted trees.  
It incorporates best practices in ML training, testing, and model evaluation [@topcuoglu_framework_2020;@teschendorff_avoiding_2019] <!-- @Begum should we cite something else here [as well]? --  Added 1 more but Jenna might have opinions on this -->
Furthermore, it provides data preprocessing steps based on the FIDDLE (FlexIble Data-Driven pipeLinE) framework outlined in Tang et al. [@tang_fiddle_2020] and post-training permutation importance steps to measure the importance of each feature in the model [@breiman_random_2001; @fisher2018models].

The framework implemented in `mikropml` is generalizable to perform ML on datasets from many different fields.
It has already been applied to microbiome data to categorize patients with colorectal cancer [@topcuoglu_framework_2020], to identify differences in genomic and clinical features associated with bacterial infections [@lapp_machine_2020], and to predict gender-based biases in academic publishing [**cite Ada’s paper**]. 


# mikropml package

The `mikropml` package has functions to preprocess the data, train ML models, and quantify feature importance. 
We also provide vignettes and an [example snakemake workflow](https://github.com/SchlossLab/mikropml-snakemake-workflow) [@koster_snakemakescalable_2012] to showcase how to run an ideal ML pipeline with multiple different train/test data splits.
The results can be visualized using functions in the package that leverage the functionality of `ggplot2` [@pedersen_ggplot2_nodate].

## Preprocessing data

We provide a function (`preprocess_data`) that preprocesses features using several different functions from the `caret` package. 
The `preprocess_data` function takes continuous and categorical data, re-factors categorical data into binary features, and provides options to normalize continuous data, remove features with near-zero variance, and keep only one instance of perfectly correlated features. 
We set the default options based on best practices implemented in FIDDLE [@tang_fiddle_2020]. 
More details on how to use the `mikropml` `preprocess_data` function can be found in the vignette [**link to preprocessing data vignette**].

## Running ML

The main function in mikropml (`run_ml`) minimally takes in a data frame including outcome and binary or continuous features, and model choice. <!-- do we also support non-preprocessed categorical features? -- We'll edit based on changes around this -->
`mikropml` currently supports logistic regression [@paul_liblinear_2017], support vector machine with a radial basis kernel [@karatzoglou_kernlab_2004], decision tree [@therneau_rpart_2019], random forest [@liaw_classication_2002], and xgBoost [@chen_xgboost_2020]. 
It randomly splits the data into train and test sets while also maintaining the distribution of the two outcomes found in the full dataset. 
It also provides the option to split the data into train and test sets based on categorical variables (e.g. batch, geographic location, etc.).
`mikropml` trains and tests the data using the `caret` R package [@kuhn_building_2008], evaluates the model using the `PRROC` R package [@grau_prroc_2015], and optionally quantifies feature importance.
The output includes the best model built based on tuning hyperparameters in an internal and repeated cross-validation step, two model evaluation metrics (area under the receiver operating characteristics curve - AUROC, and area under the precision recall curve - AUPRC), and optional feature importances (Figure 1). 
The quantification of feature importance using permutation allows the calculation of the decrease in the model's prediction performance after breaking the relationship between the feature and the true outcome, and is thus particularly useful for model interpretation [@topcuoglu_framework_2020]. 
Our vignette [**link to vignette**] contains a comprehensive tutorial on how to use the `run_ml` function.

![Figure 1. mikropml pipeline](mikRopML-pipeline.png){width=100%}

## Ideal workflow for running mikropml with many different train/test splits

To investigate the variation in model performance depending on the train and test set used [@topcuoglu_framework_2020; @lapp_machine_2020], we provide examples of how to run the `run_ml` function many times with different train/test splits and how to get summary information about model performance on your local computer [**link to parallel vignette**] or on a high-computing cluster using a snakemake workflow [**link to snakemake workflow example**]. 

## Plotting ML results 

One particularly important aspect of ML is hyperparameter tuning. 
Practitioners must explore a range of hyperparameter possibilities to pick the ideal value for the model and dataset.
Therefore, we provide a function (**insert name of function**) to plot the cross-validation AUROC of models built using different train/test splits to evaluate if we are exhausing our hyperparameter search range to pick the ideal one. 
We also provide summary plots of test AUROC and AUPRC values for the many train/test splits using (**insert name of function**).

## Dependencies

mikropml is written in R [@r_core_team_r_2020] and depends on several packages: `PRROC` [@grau_prroc_2015], `dplyr` [@wickham_dplyr_2020], `rlang` [@henry_rlang_2020] and `caret` [@kuhn_building_2008]. 
The ML algorithms supported by `mikropml` require:
`LiblineaR` foe logistic regression [@paul_liblinear_2017], `rpart2` [@therneau_rpart_2019] for decision trees, `randomForest` [@liaw_classication_2002] for random forest, `xgboost` [@chen_xgboost_2020] for xgBoost, and `kernlab` [@karatzoglou_kernlab_2004] for support vector machines. 
We also allow for parallelization of cross-validation and other steps using the `foreach`, `doFuture`, `future.apply`, and `future` packages [@bengtsson_futureapply_2020].
Finally, we use `ggplot2` for plotting [@pedersen_ggplot2_nodate].

# Acknowledgements

We thank members of the Schloss Lab who participated in code clubs related to the initial development of the pipeline.

# Funding
Salary support for PDS came from NIH grant 1R01CA215574.
KLS received support from the NIH Training Program in Bioinformatics (T32 GM070449).
ZL received support from the National Science Foundation Graduate Research Fellowship Program under Grant No. DGE 1256260. 
Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation.

# Author contributions

BT, ZL, and KLS conceptualized the study and created the package.
BT, ZL, JW, and PDS developed methodology. 
PDS, ES, and JW supervised the project. 
BT, ZL, and KLS wrote the original draft. 
All authors reviewed and edited the manuscript.

# Conflicts of interest
<!--TODO Add conflicts of interest here -->

# References
