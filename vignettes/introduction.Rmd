---
title: "Introduction to mikropml"
author: "Zena Lapp"
output: rmarkdown::html_vignette
bibliography: paper.bib
vignette: >
  %\VignetteIndexEntry{Introduction to mikropml}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

The goal of `mikropml` is to make supervised machine learning (ML) easy for you 
to run while not losing any of the rigor associated with robust machine learning pipelines. 
All you need to run ML is one function: `run_ml()`. 
We've selected sane default arguments related to best practices [@topcuoglu_framework_2020], 
but we allow you to change those arguments to tailor `run_ml()` to the needs of your data.

This document takes you through all of the `run_ml()` inputs, 
both required and optional, as well as the outputs.

In summary, you provide:

- A dataset with an outcome column and feature columns (rows are samples)
- Model choice (i.e. method)

And the function outputs:

- The trained model
- Model performance metrics
- (Optional) feature importance metrics

# It's running so slow!

Since I assume a lot of you won't read this entire vignette, 
I'm going to say this at the beginning. 
If the `run_ml()` function is running super slow, you should consider parallelizing. See `vignette(parallel)` for examples.

# Understanding the inputs

## The input data

The input data to `run_ml()` is a dataframe where each row is a sample or observation. 
One column (assumed to be the first) is the outcome of interest, 
and all of the other columns are the features. 
We package `otu_mini` as an example dataset with `mikropml`.

```{r}
#install.packages("devtools")
#devtools::install_github("SchlossLab/mikropml")
library(mikropml)
head(otu_mini)
```

Here, `dx` is the outcome column (normal or cancer), and there are 3 features (`Otu00001` through `Otu00003`). 
Because there are only 2 outcomes, we will be performing binary classification in the majority of the examples below. 
At the bottom, we will also briefly provide examples of multi-class and continuous outcomes. 
As you'll see, you run them in the same way as for binary classification!

The feature columns are the amount of each [Operational Taxonomic Unit (OTU; proxy for species)](https://en.wikipedia.org/wiki/Operational_taxonomic_unit) 
in microbiome samples from patients with cancer and without cancer. 
The goal is to predict cancer or not based on an individual's microbiome. 
No need to understand exactly what that means, 
but if you're interested you can read more about it from the original paper [@topcuoglu_framework_2020].

For real machine learning you'll want to use more features, 
but for the purposes of this vignette we'll stick with this dataset so everything runs faster.

## The methods we support

All of the methods we use are supported by [`caret`](https://topepo.github.io/caret/), which is the package we use to train our machine learning models. 

The methods we have tested are: 

- Logistic/multiclass/linear regression (`"glmnet"`)
- Random forest (`"rf"`)
- Decision tree (`"rpart2"`)
- Support vector machine with a linear basis kernel (`"svmRadial"`)
- xgBoost (`"xgbTree"`)

For documentation on these methods, as well as many others, you can look at the 
[available models](https://topepo.github.io/caret/available-models.html) 
(or see [here](https://topepo.github.io/caret/train-models-by-tag.html) for a list by tag). 
While we have not vetted the other models used by `caret`, our function is general enough that others might work.
While we can't promise that we can help with other models, 
feel free to [open an issue on GitHub](https://github.com/SchlossLab/mikropml/issues) if you have questions about other models and we _might_ be able to help.

We will focus on `glmnet` here, then cover a few other examples towards the end.

# Before running ML

Before you run ML, you should consider preprocessing your data, 
either on your own or with the `preprocess_data()` function. 
You can learn more about this in the preprocessing vignette: `vignette(preprocess)`. 
Running it with default options will center and scale the features of our dataset.

```{r}
preproc <- preprocess_data(dataset = otu_mini, 
                           outcome_colname = 'dx')
dat <- preproc$dat_transformed
```

We'll use `dat` for the following examples.

# The simplest way to `run_ml()`

As mentioned above, the minimal input is your dataset (`dataset`) and the machine learning model you want to use (`method`).

We may also want to provide:

- The outcome column name. By default it will pick the first column, but it's best practice to specify it explicitly.
- A seed so that the results will be reproducible, and so that you get the same results as those you see here (i.e have the same train/test split). 

Say we want to use `glmnet`.Then, run ML with:

```{r}
results <- run_ml(dat, 
                  'glmnet', 
                  outcome_colname = 'dx', 
                  kfold = 3, # TODO once we make otu_mini larger, i'll get rid of kfold in this example
                  seed = 2019) 
```

You'll notice a few things:

1. It takes a little while to run. This is because of some of the parameters we use. 
1. There is a message stating that 'dx' is being used as the outcome column. This is what we want, but it's a nice sanity check!
1. There was a warning. Don't worry about this warning right now - it just means that some of the hyperparameters aren't a good fit - but if you're interested in learning more, head over to the hyperparameter tuning vignette to learn more [**link to these once we have the vignette**].

Now, let's dig into the output a bit.
The results is a list of 4 things:

```{r}
names(results)
```

`trained_model` is the trained model from `caret`. 
There is a bunch of info in this that we won't get into, 
because you can learn more from the `caret` documentation. 
**TODO: link to relevant part of caret docs**

```{r}
names(results$trained_model)
```

`test_data` is the data that was used for testing. 
In machine learning, it's always important to split your data in to training and testing, and we do that for you in `run_ml()`. 
The training data is used to build the model and the test data is used to evaluate how well the model performs.

```{r}
head(results$test_data)
```

`performance` is a dataframe of (mainly) performance metrics (1 column for cross-validation performance metric, several for test performance metrics, and 2 columns at the end with ML method and seed):

```{r}
results$performance
```

Usually for logistic regression, area under the receiver-operator characteristic curve (AUC) is a useful meric to evaluate model performance. 
Because of that, it's the default that we use for `mikropml`. 
Below you can find more information about using other performance metrics.

`cv_metric_AUC` is the AUC for the cross-validation folds for the training data. 
This gives us a sense of how good the model is on the training data.

Most of the other columns are performance metrics for the test data — the data that wasn't used to build the model.
Here, you can see that the AUC for the test data is not much above 0.5, 
suggesting that this model does not predict much better than chance, 
and that the model is overfitted because the cross-validation AUC is much higher than the training AUC. 
This isn't too surprising since we're using so few features.
The default option also provides a number of other performance metrics that you might be interested in, 
including area under the precision-recall curve (prAUC).

At the very end, there are columns for the method and seed (if you set one)
to help with combining results from multiple runs (see `vignette('parallel')`).

`feature_importance` has information about feature importances if 
`find_feature_importance = TRUE` (the default is `FALSE`). 
Since we used the defaults, there's nothing here:

```{r}
results$feature_importance
```

# Customizing parameters

There are a few arguments that allow you to change how you run ML. 
We've chosen reasonable defaults for you, but we encourage you to change these 
if you think something else would be better for your data.

## Changing `kfold`, `cv_times`, and `training_frac`

- `kfold`: The number of folds to run for cross-validation (default: 5).
- `cv_times`: The number of times to run cross-validation (default: 100).
- `training_frac`: The fraction of data for the training set (default: 0.8). The rest of the data is used for testing.

Here's an example where we change some of the default parameters:

```{r, eval}
results_custom <- run_ml(dat, 
                         'glmnet', 
                         kfold = 2, 
                         cv_times = 5,
                         training_frac = 0.5, 
                         seed = 2019)
```

You might have noticed that this one ran faster — that's because we reduced `kfold` and `cv_times`. 
This is okay for testing things out, but in general it's better to have these numbers be larger [@topcuoglu_framework_2020]. 

## Changing the performance metric

There are two arguments that allow you to change what performance metric to use for model evaluation, a
nd what performance metrics to calculate on the test data.

`perf_metric_function` is the function to use to calculate the performance metrics. 

The default for classification is `caret::multiClassSummary()` and the default for regression is `caret::defaultSummary()`. 
We'd suggest not changing this unless you really know what you're doing. 

`perf_metric_name` is the column name from the output of `perf_metric_function` to use as the performance metric. 
We chose reasonable defaults (AUC for binary, logLoss for multiclass, and RMSE for continuous), 
but the default functions calculate a bunch of different performance metrics, 
so you can choose a different one if you'd like. 

The default performance metrics available for classificatoin are:

```{r, echo=FALSE}
c("logLoss", "AUC", "prAUC", "Accuracy", "Kappa", "Mean_F1", "Mean_Sensitivity", "Mean_Specificity", "Mean_Pos_Pred_Value", "Mean_Neg_Pred_Value", "Mean_Precision", "Mean_Recall", "Mean_Detection_Rate", "Mean_Balanced_Accuracy")
```

The default performance metrics available for regression are:

```{r, echo=FALSE}
c("RMSE", "Rsquared", "MAE")
```

Here's an example using prAUC instead of AUC:

```{r}
results_pr <- run_ml(dat, 'glmnet', kfold = 2, cv_times = 5, perf_metric_name = 'prAUC', seed = 2019)
```

You'll see that the cross-validation metric is prAUC, instead of the default AUC:

```{r}
results_pr$performance
```

## Using groups

The optional `groups` is a vector of groups to keep together when splitting the data into train and test sets and for cross-validation. 
This can be a little finnicky depending on how many samples you have and how many groups you have, 
but sometimes it's important to split up the data based on group instead of just randomly.
This allows you to control for similarities within groups that you don't want to skew your predictions (i.e. batch effects). 
For example, with biological data you may have samples collected from multiple hospitals, 
and you might like to keep observations from the same hospital in the same split.

Here's an example where we split the data into train/test sets based on a group:

```{r, eval = FALSE}
# make random groups
set.seed(2019)
grps <- sample(LETTERS[1:8], nrow(dat),replace=TRUE)
results_grp <- run_ml(dat, 'glmnet', kfold = 2, cv_times = 5, training_frac = 0.8, groups = grps, seed = 2019)
```

The one difference here is it tells you how much of the data is in the training set. 
This is because it won't be exactly what you specify with `training_frac`, 
since you have to include all of one group in either the training set _or_ the test set. 

# Finding feature importance

To find which features are contributing to predictive power, 
you can use `find_feature_importance = TRUE`. 
Feature importance is decribed in [@topcuoglu_framework_2020]. 
Briefly, it permutes each of the features individually (or correlated ones together) and evaluates how much the performance metric decreases. 
The more performance decreases when the feature is randomly shuffled, the more important that feature is. 
The default is `FALSE` because it takes a while to run and is only useful if you
want to know what features are important in predicting your outcome. 

Let's look at some feature importance results:

```{r}
results_imp <- run_ml(dat, 
                      'glmnet',
                      kfold = 2, 
                      cv_times = 5, 
                      find_feature_importance = TRUE, 
                      seed = 2019)
```

Now, we can check out the feature importances:

```{r}
results_imp$feature_importance
```

There are several columns:

1. `perf_metric`: The performance metric of the permuted feature.
1. `perf_metric_diff`: The difference between the performance metric for the true and permuted data.
1. `names`: The feature that was permuted.
1. `method`: The ML method used.
1. `perf_metric_name`: The peformance metric used.
1. `seed`: The seed (if set). 

As you can see here, the differences are negligible here (close to zero), which makes sense since our model isn't great. If you're interested in feature importance, it's especially uesful to run multiple different train/test splits, as described here [**Link to snakemake workflow**].

You can also choose to permute correalted features together using `corr_thresh` (default: 1). Any features that are above the correlation threshold are permuted together; i.e. perfectly correlated features are permuted together when using the default value.

```{r}
results_imp_corr <- run_ml(dat, 
                           'glmnet', 
                           kfold = 2, 
                           cv_times = 5, 
                           find_feature_importance = TRUE,
                           corr_thresh = 0.2,
                           seed = 2019)
results_imp_corr$feature_importance
```

You can see what features were permuted together in the `names` column. Here all 3 features were permuted together (which doesn't really make sense, but it's just an example).

# Tuning hyperparameters (using the `hyperparameter` argument)

This is important, so we have a whole vignette about them. 
The bottom line is we provide default hyperparameters that you can start with, but it's important to tune your hyperparameters. 
For more information about what the default hyperparameters are, and how to tune hyperparameters, 
check out the hyperparameter tuning vignette [**link to this once we have the vignette**]. 

# Other models

Here are examples of how to run the other models. 
The output for all of them is very similar, so we won't go into those details.

## Random forest

```{r, eval = FALSE}
results_rf <- run_ml(dat, 
                     'rf', 
                     kfold = 2,
                     cv_times = 5, 
                     seed = 2019)
```

You can also change the number of trees to use for random forest (`ntree`; default: 1000). This can't be tuned using `caret`.

```{r, eval = FALSE}
results_rf_nt <- run_ml(dat, 
                        'rf', 
                        kfold = 2, 
                        cv_times = 5, 
                        ntree = 10, 
                        seed = 2019)
```

## Decision tree

```{r, eval = FALSE}
results_dt <- run_ml(dat, 
                     'rpart2', 
                     kfold = 2, 
                     cv_times = 5, 
                     seed = 2019)
```

## SVM

```{r, eval = FALSE}
results_svm <- run_ml(dat, 
                      'svmRadial', 
                      kfold = 2, 
                      cv_times = 5, 
                      seed = 2019)
```

Don't worry about the maximum number of iterations reached. [**TODO Check if it's an issue**]

# Other data

## Multiclass data

Here's an example of running multiclass data:

```{r}
results_multi <- run_ml(otu_mini_multi, 
                        'glmnet', 
                        kfold = 2, 
                        cv_times = 5, 
                        seed = 2019)
```

The performance metrics are slightly different, 
but the format of everything else is the same:

```{r}
results_multi$performance
```

## Continuous data

And here's an example for running continuous data:

```{r}
results_cont <- run_ml(dat, 
                       'glmnet', 
                       outcome_colname = 'Otu00001', 
                       kfold = 2, 
                       cv_times = 5, 
                       seed = 2019)
```

You can ignore the warning, because we want it to be continuous data. 

Again, the performance metrics are slightly different, 
but the format of the rest is the same:

```{r}
results_cont$performance
```

# References
